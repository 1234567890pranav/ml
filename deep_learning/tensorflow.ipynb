{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 9 â€“ Up and running with TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The evaluation in instant traditionally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = x*x*y + y + 2\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "# Check the tensorflow version\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"tensorflow\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and running a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Better way\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another better way\n",
    "\n",
    "init = tf.global_variables_initializer() # prepare an init node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actually initialize all the variables\n",
    "    result = f.eval()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.framework.ops.Graph object at 0x7f39c69a66a0>\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()\n",
    "\n",
    "print(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "\n",
    "print(x2.graph is graph)\n",
    "print(x2.graph is tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To reset the default group\n",
    "\n",
    "reset_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lifecycle of a Node Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(y.eval())  # 10\n",
    "    print(z.eval())  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# Run previous code efficiently\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z])\n",
    "    print(y_val)  # 10\n",
    "    print(z_val)  # 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.21428571],\n",
       "       [1.07142857]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,5/3],[1, 1/9]])\n",
    "y = np.array([[12/3],[21/9]])\n",
    "theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.7185181e+01],\n",
       "       [ 4.3633747e-01],\n",
       "       [ 9.3952334e-03],\n",
       "       [-1.0711310e-01],\n",
       "       [ 6.4479220e-01],\n",
       "       [-4.0338000e-06],\n",
       "       [-3.7813708e-03],\n",
       "       [-4.2348403e-01],\n",
       "       [-4.3721911e-01]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate theta using normal equation in TensorFlow\n",
    "# We will use housing dataset of end-to-end project\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data]\n",
    "\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()\n",
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare it with pure NumPy\n",
    "# Calculate theta using normal equation in NumPy\n",
    "\n",
    "X = housing_data_plus_bias\n",
    "y = housing.target.reshape(-1, 1)\n",
    "theta_numpy = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "\n",
    "print(theta_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.69419202e+01]\n",
      " [ 4.36693293e-01]\n",
      " [ 9.43577803e-03]\n",
      " [-1.07322041e-01]\n",
      " [ 6.45065694e-01]\n",
      " [-3.97638942e-06]\n",
      " [-3.78654265e-03]\n",
      " [-4.21314378e-01]\n",
      " [-4.34513755e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Compare it with pure NumPy\n",
    "# Calculate theta using Scikit-Learn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing.data, housing.target.reshape(-1, 1))\n",
    "\n",
    "print(np.r_[lin_reg.intercept_.reshape(-1, 1), lin_reg.coef_.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00  6.60969987e-17  5.50808322e-18  6.60969987e-17\n",
      " -1.06030602e-16 -1.10161664e-17  3.44255201e-18 -1.07958431e-15\n",
      " -8.52651283e-15]\n",
      "[ 0.38915536  0.36424355  0.5116157  ... -0.06612179 -0.06360587\n",
      "  0.01359031]\n",
      "0.11111111111111005\n",
      "(20640, 9)\n"
     ]
    }
   ],
   "source": [
    "# Step 1- Normalize the feature vector list using Scikit-Learn\n",
    "# Gradient Descent requires scaling the feature vectors first\n",
    "# We could do this using TF, but let's just use Scikit-Learn for now.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_housing_data = scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]\n",
    "\n",
    "print(scaled_housing_data_plus_bias.mean(axis=0))\n",
    "print(scaled_housing_data_plus_bias.mean(axis=1))\n",
    "print(scaled_housing_data_plus_bias.mean())\n",
    "print(scaled_housing_data_plus_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually computing the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.71450067\n",
      "Epoch 200 MSE = 0.5667049\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.53650916\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2.0685523 ],\n",
       "       [ 0.8874027 ],\n",
       "       [ 0.14401656],\n",
       "       [-0.34770885],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145283],\n",
       "       [-0.6375278 ]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2- Manually computing the gradient\n",
    "# The Code is self-explanatory, except for a few new elements\n",
    "\n",
    "'''\n",
    "\n",
    "1. random_uniform() - The random_uniform() function creates a node in the graph that will generate a tensor containing\n",
    "random values, given its shape and value range, much like NumPyâ€™s rand() function.\n",
    "\n",
    "2. assign() - The assign() function creates a node that will assign a new value to a variable. In this case, it\n",
    "implements the Batch Gradient Descent step Î¸(next step) = Î¸ â€“ Î·âˆ‡Î¸MSE(Î¸).\n",
    "\n",
    "3. The main loop executes the training step over and over again (n_epochs times), and every 100 iterations it prints \n",
    "out the current Mean Squared Error (mse). We can see the MSE go down at every iteration\n",
    "\n",
    "'''\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "gradients = 2/m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc472450ef0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkyIQQggPJ0UghBAe7v8AmL4ZZCAV2VUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc47459b828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us understand the differentiation\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate x\n",
    "x = np.linspace(-10, 10, 100)\n",
    "# Compute y\n",
    "y = x*x\n",
    "\n",
    "# plot the parabola\n",
    "plt.plot(x, y)\n",
    "\n",
    "#Lets draw a tangent at x0 = 5\n",
    "x0 = 5\n",
    "m = slope = 2*x0 # d(x^2)/dx\n",
    "\n",
    "y1 = m*x-25\n",
    "plt.plot(x, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "# The above method was possible only for the cases where computing differential was easy. We can use autodiff.\n",
    "# Lets try. Now, lets compute the \n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "x = tf.constant(5.0)\n",
    "y = tf.square(x)\n",
    "z = tf.gradients(y, x)\n",
    "with tf.Session() as s:\n",
    "    print(z[0].eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Same as above except for the `gradients = ...` line:\n",
    "# Compute gradients using autodiff\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = tf.gradients(mse, [theta])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.161543\n",
      "Epoch 100 MSE = 0.7145006\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.5555719\n",
      "Epoch 400 MSE = 0.5488112\n",
      "Epoch 500 MSE = 0.5436362\n",
      "Epoch 600 MSE = 0.5396294\n",
      "Epoch 700 MSE = 0.5365092\n",
      "Epoch 800 MSE = 0.5340678\n",
      "Epoch 900 MSE = 0.5321474\n",
      "Best theta:\n",
      "[[ 2.0685525 ]\n",
      " [ 0.8874027 ]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.6614528 ]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "training_op = tf.assign(theta, theta - learning_rate * gradients)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the partial derivatives of the following function with regards to `a` and `b`\n",
    "\n",
    "def my_func(a, b):\n",
    "    z = 0\n",
    "    for i in range(100):\n",
    "        z = a * np.cos(z + i) + z * np.sin(b - i)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21253923284754916"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_func(0.2, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a = tf.Variable(0.2, name=\"a\")\n",
    "b = tf.Variable(0.3, name=\"b\")\n",
    "z = tf.constant(0.0, name=\"z0\")\n",
    "for i in range(100):\n",
    "    z = a * tf.cos(z + i) + z * tf.sin(b - i)\n",
    "\n",
    "grads = tf.gradients(z, [a, b])\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the function at $a=0.2$ and $b=0.3$, and the partial derivatives at that point with regards to $a$ and with regards to $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.212537\n",
      "[-1.1388494, 0.19671395]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    print(z.eval())\n",
    "    print(sess.run(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `GradientDescentOptimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n",
      "Best theta:\n",
      "[[ 2.06855249]\n",
      " [ 0.88740271]\n",
      " [ 0.14401658]\n",
      " [-0.34770882]\n",
      " [ 0.36178368]\n",
      " [ 0.00393811]\n",
      " [-0.04269556]\n",
      " [-0.66145277]\n",
      " [-0.6375277 ]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a momentum optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")\n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best theta:\n",
      "[[ 2.06855798]\n",
      " [ 0.82962859]\n",
      " [ 0.11875337]\n",
      " [-0.26554456]\n",
      " [ 0.30571091]\n",
      " [-0.00450251]\n",
      " [-0.03932662]\n",
      " [-0.89986444]\n",
      " [-0.87052065]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feeding data to the training algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.  7.  8.]]\n",
      "[[  9.  10.  11.]\n",
      " [ 12.  13.  14.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's implement mini-batch using placeholder nodes\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Let's make X and y as placeholder nodes\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "\n",
    "\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the batch size and compute the total number of batches\n",
    "# Define function to fetch batch\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    np.random.seed(epoch * n_batches + batch_index) \n",
    "    indices = np.random.randint(m, size=batch_size)\n",
    "    X_batch = scaled_housing_data_plus_bias[indices]\n",
    "    y_batch = housing.target.reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is execution phase\n",
    "# Fetch the mini-batches one by one then provide the value of x and y via the feed_dict parameter\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and restoring a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 MSE = 9.16154\n",
      "Epoch 100 MSE = 0.714501\n",
      "Epoch 200 MSE = 0.566705\n",
      "Epoch 300 MSE = 0.555572\n",
      "Epoch 400 MSE = 0.548812\n",
      "Epoch 500 MSE = 0.543636\n",
      "Epoch 600 MSE = 0.539629\n",
      "Epoch 700 MSE = 0.536509\n",
      "Epoch 800 MSE = 0.534068\n",
      "Epoch 900 MSE = 0.532147\n"
     ]
    }
   ],
   "source": [
    "# Let's save a model\n",
    "# Code for GradientDescentOptimizer\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_epochs = 1000                                                                       \n",
    "learning_rate = 0.01                                                                  \n",
    "\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name=\"X\")            \n",
    "y = tf.constant(housing.target.reshape(-1, 1), dtype=tf.float32, name=\"y\")           \n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")                                      \n",
    "error = y_pred - y                                                                    \n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")                                    \n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)            \n",
    "training_op = optimizer.minimize(mse)                                                \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver() # Create a Saver node\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch\", epoch, \"MSE =\", mse.eval())                            \n",
    "            save_path = saver.save(sess, \"/tmp/my_model.ckpt\") # Save checkpoint during training\n",
    "        sess.run(training_op)\n",
    "    \n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, \"/tmp/my_model_final.ckpt\") # Save model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.06855249],\n",
       "       [ 0.88740271],\n",
       "       [ 0.14401658],\n",
       "       [-0.34770882],\n",
       "       [ 0.36178368],\n",
       "       [ 0.00393811],\n",
       "       [-0.04269556],\n",
       "       [-0.66145277],\n",
       "       [-0.6375277 ]], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Restore model\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")\n",
    "    best_theta_restored = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if restored theta is same as the saved theta\n",
    "\n",
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we want to have a saver that loads and restores `theta` with a different name, such as `\"weights\"`\n",
    "\n",
    "saver = tf.train.Saver({\"weights\": theta})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the saver also saves the graph structure itself in a second file with the extension `.meta`. You can use the function `tf.train.import_meta_graph()` to restore the graph structure. This function loads the graph into the default graph and returns a `Saver` that can then be used to restore the graph state (i.e., the variable values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "# notice that we start with an empty graph.\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"/tmp/my_model_final.ckpt.meta\")  # this loads the graph structure\n",
    "theta = tf.get_default_graph().get_tensor_by_name(\"theta:0\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/my_model_final.ckpt\")  # this restores the graph's state\n",
    "    best_theta_restored = theta.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(best_theta, best_theta_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that you can import a pretrained model without having to have the corresponding Python code to build the graph. This is very handy when you keep tweaking and saving your model: you can load a previously saved model without having to search for the version of the code that built it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the graph\n",
    "## inside Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 9\\n          }\\n        }\\n        tensor_content: &quot;<stripped 743040 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 20640\\n          }\\n          dim {\\n            size: 1\\n          }\\n        }\\n        tensor_content: &quot;<stripped 82560 bytes>&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\t\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;random_uniform/max&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;random_uniform/RandomUniform&quot;\\n  input: &quot;random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;random_uniform/mul&quot;\\n  input: &quot;random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 9\\n        }\\n        dim {\\n          size: 1\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;theta/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;predictions&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;predictions&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Square&quot;\\n  op: &quot;Square&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\000\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mse&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;Square&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\001\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/mse_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/mse_grad/Reshape&quot;\\n  input: &quot;gradients/mse_grad/Tile/multiples&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 20640.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/mse_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/mse_grad/Tile&quot;\\n  input: &quot;gradients/mse_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul/x&quot;\\n  op: &quot;Const&quot;\\n  input: &quot;^gradients/mse_grad/truediv&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/Square_grad/mul/x&quot;\\n  input: &quot;sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Square_grad/mul_1&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/mse_grad/truediv&quot;\\n  input: &quot;gradients/Square_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Shape_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\240P\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  op: &quot;BroadcastGradientArgs&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Sum&quot;\\n  input: &quot;gradients/sub_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Sum_1&quot;\\n  op: &quot;Sum&quot;\\n  input: &quot;gradients/Square_grad/mul_1&quot;\\n  input: &quot;gradients/sub_grad/BroadcastGradientArgs:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Neg&quot;\\n  op: &quot;Neg&quot;\\n  input: &quot;gradients/sub_grad/Sum_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/Reshape_1&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/sub_grad/Neg&quot;\\n  input: &quot;gradients/sub_grad/Shape_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/Reshape_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/sub_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/sub_grad/Reshape_1&quot;\\n  input: &quot;^gradients/sub_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/sub_grad/Reshape_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  input: &quot;theta/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/sub_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/predictions_grad/MatMul_1&quot;\\n  input: &quot;^gradients/predictions_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/predictions_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_theta/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;gradients/predictions_grad/tuple/control_dependency_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_theta/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^theta/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;theta&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;theta&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;theta&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@theta&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                                        \n",
    "    sess.run(init)                                                               \n",
    "\n",
    "    for epoch in range(n_epochs):                                                 \n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.07033372],\n",
       "       [ 0.86371452],\n",
       "       [ 0.12255151],\n",
       "       [-0.31211874],\n",
       "       [ 0.38510373],\n",
       "       [ 0.00434168],\n",
       "       [-0.01232954],\n",
       "       [-0.83376896],\n",
       "       [-0.80304712]], dtype=float32)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0, seed=42), name=\"theta\")\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\") as scope:\n",
    "    error = y_pred - y\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batches + batch_index\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "    best_theta = theta.eval()\n",
    "\n",
    "file_writer.flush()\n",
    "file_writer.close()\n",
    "print(\"Best theta:\")\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(error.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(mse.op.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "a1 = tf.Variable(0, name=\"a\")      # name == \"a\"\n",
    "a2 = tf.Variable(0, name=\"a\")      # name == \"a_1\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param\"\n",
    "    a3 = tf.Variable(0, name=\"a\")  # name == \"param/a\"\n",
    "\n",
    "with tf.name_scope(\"param\"):       # name == \"param_1\"\n",
    "    a4 = tf.Variable(0, name=\"a\")  # name == \"param_1/a\"\n",
    "\n",
    "for node in (a1, a2, a3, a4):\n",
    "    print(node.op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ugly flat code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0., name=\"relu1\")\n",
    "relu2 = tf.maximum(z1, 0., name=\"relu2\")  # Oops, cut&paste error! Did you spot it?\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better, using a function to build the ReLUs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    w_shape = (int(X.get_shape()[1]), 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0., name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu1\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better using name scopes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")    # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                             # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                      # not shown\n",
    "        return tf.maximum(z, 0., name=\"max\")                          # not shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu2\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharing a `threshold` variable the classic way, by defining it outside of the `relu()` function then passing it as a parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X, threshold) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.name_scope(\"relu\"):\n",
    "        if not hasattr(relu, \"threshold\"):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown in the book\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, relu.threshold, name=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\", reuse=True):\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"relu\") as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable(\"threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\", reuse=True):\n",
    "        threshold = tf.get_variable(\"threshold\")\n",
    "        w_shape = int(X.get_shape()[1]), 1                          # not shown\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "        b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"relu\"):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for relu_index in range(5)]\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu6\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    with tf.variable_scope(\"relu\"):\n",
    "        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n",
    "        w_shape = (int(X.get_shape()[1]), 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope(\"\", default_name=\"\") as scope:\n",
    "    first_relu = relu(X)     # create the shared variable\n",
    "    scope.reuse_variables()  # then reuse it\n",
    "    relus = [first_relu] + [relu(X) for i in range(4)]\n",
    "output = tf.add_n(relus, name=\"output\")\n",
    "\n",
    "file_writer = tf.summary.FileWriter(\"logs/relu8\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "def relu(X):\n",
    "    threshold = tf.get_variable(\"threshold\", shape=(),\n",
    "                                initializer=tf.constant_initializer(0.0))\n",
    "    w_shape = (int(X.get_shape()[1]), 1)                        # not shown in the book\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")  # not shown\n",
    "    b = tf.Variable(0.0, name=\"bias\")                           # not shown\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")                    # not shown\n",
    "    return tf.maximum(z, threshold, name=\"max\")\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = []\n",
    "for relu_index in range(5):\n",
    "    with tf.variable_scope(\"relu\", reuse=(relu_index >= 1)) as scope:\n",
    "        relus.append(relu(X))\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter(\"logs/relu9\", tf.get_default_graph())\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "with tf.variable_scope(\"my_scope\"):\n",
    "    x0 = tf.get_variable(\"x\", shape=(), initializer=tf.constant_initializer(0.))\n",
    "    x1 = tf.Variable(0., name=\"x\")\n",
    "    x2 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"my_scope\", reuse=True):\n",
    "    x3 = tf.get_variable(\"x\")\n",
    "    x4 = tf.Variable(0., name=\"x\")\n",
    "\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):\n",
    "    x5 = tf.get_variable(\"my_scope/x\")\n",
    "\n",
    "print(\"x0:\", x0.op.name)\n",
    "print(\"x1:\", x1.op.name)\n",
    "print(\"x2:\", x2.op.name)\n",
    "print(\"x3:\", x3.op.name)\n",
    "print(\"x4:\", x4.op.name)\n",
    "print(\"x5:\", x5.op.name)\n",
    "print(x0 is x3 and x3 is x5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first `variable_scope()` block first creates the shared variable `x0`, named `my_scope/x`. For all operations other than shared variables (including non-shared variables), the variable scope acts like a regular name scope, which is why the two variables `x1` and `x2` have a name with a prefix `my_scope/`. Note however that TensorFlow makes their names unique by adding an index: `my_scope/x_1` and `my_scope/x_2`.\n",
    "\n",
    "The second `variable_scope()` block reuses the shared variables in scope `my_scope`, which is why `x0 is x3`. Once again, for all operations other than shared variables it acts as a named scope, and since it's a separate block from the first one, the name of the scope is made unique by TensorFlow (`my_scope_1`) and thus the variable `x4` is named `my_scope_1/x`.\n",
    "\n",
    "The third block shows another way to get a handle on the shared variable `my_scope/x` by creating a `variable_scope()` at the root scope (whose name is an empty string), then calling `get_variable()` with the full name of the shared variable (i.e. `\"my_scope/x\"`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "text = np.array(\"Do you want some cafÃ©?\".split())\n",
    "text_tensor = tf.constant(text)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(text_tensor.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Home-Made Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() + self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        return self.a.evaluate() * self.b.evaluate()\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", f.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing gradients\n",
    "### Mathematical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dx = Mul(Const(2), Mul(x, y))  # df/dx = 2xy\n",
    "df_dy = Add(Mul(x, x), Const(1))  # df/dy = xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradients(func, vars_list, eps=0.0001):\n",
    "    partial_derivatives = []\n",
    "    base_func_eval = func.evaluate()\n",
    "    for var in vars_list:\n",
    "        original_value = var.value\n",
    "        var.value = var.value + eps\n",
    "        tweaked_func_eval = func.evaluate()\n",
    "        var.value = original_value\n",
    "        derivative = (tweaked_func_eval - base_func_eval) / eps\n",
    "        partial_derivatives.append(derivative)\n",
    "    return partial_derivatives\n",
    "\n",
    "df_dx, df_dy = gradients(f, [x, y])\n",
    "print(\"df/dx(3,4) =\", df_dx)\n",
    "print(\"df/dy(3,4) =\", df_dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symbolic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Const.derive = lambda self, var: Const(0)\n",
    "Var.derive = lambda self, var: Const(1) if self is var else Const(0)\n",
    "Add.derive = lambda self, var: Add(self.a.derive(var), self.b.derive(var))\n",
    "Mul.derive = lambda self, var: Add(Mul(self.a, self.b.derive(var)), Mul(self.a.derive(var), self.b))\n",
    "\n",
    "x = Var(3.0, name=\"x\")\n",
    "y = Var(4.0, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "df_dx = f.derive(x)  # 2xy\n",
    "df_dy = f.derive(y)  # xÂ² + 1\n",
    "print(\"df/dx(3,4) =\", df_dx.evaluate())\n",
    "print(\"df/dy(3,4) =\", df_dy.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation (autodiff) â€“ forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DualNumber(object):\n",
    "    def __init__(self, value=0.0, eps=0.0):\n",
    "        self.value = value\n",
    "        self.eps = eps\n",
    "    def __add__(self, b):\n",
    "        return DualNumber(self.value + self.to_dual(b).value,\n",
    "                          self.eps + self.to_dual(b).eps)\n",
    "    def __radd__(self, a):\n",
    "        return self.to_dual(a).__add__(self)\n",
    "    def __mul__(self, b):\n",
    "        return DualNumber(self.value * self.to_dual(b).value,\n",
    "                          self.eps * self.to_dual(b).value + self.value * self.to_dual(b).eps)\n",
    "    def __rmul__(self, a):\n",
    "        return self.to_dual(a).__mul__(self)\n",
    "    def __str__(self):\n",
    "        if self.eps:\n",
    "            return \"{:.1f} + {:.1f}Îµ\".format(self.value, self.eps)\n",
    "        else:\n",
    "            return \"{:.1f}\".format(self.value)\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "    @classmethod\n",
    "    def to_dual(cls, n):\n",
    "        if hasattr(n, \"value\"):\n",
    "            return n\n",
    "        else:\n",
    "            return cls(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$3 + (3 + 4 \\epsilon) = 6 + 4\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3 + DualNumber(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(3 + 4Îµ)\\times(5 + 7Îµ) = 3 \\times 5 + 3 \\times 7Îµ + 4Îµ \\times 5 + 4Îµ \\times 7Îµ = 15 + 21Îµ + 20Îµ + 28Îµ^2 = 15 + 41Îµ + 28 \\times 0 = 15 + 41Îµ$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DualNumber(3, 4) * DualNumber(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0)\n",
    "y.value = DualNumber(4.0)\n",
    "\n",
    "f.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.value = DualNumber(3.0, 1.0)  # 3 + Îµ\n",
    "y.value = DualNumber(4.0)       # 4\n",
    "\n",
    "df_dx = f.evaluate().eps\n",
    "\n",
    "x.value = DualNumber(3.0)       # 3\n",
    "y.value = DualNumber(4.0, 1.0)  # 4 + Îµ\n",
    "\n",
    "df_dy = f.evaluate().eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ Reverse mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Const(object):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        pass\n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "class Var(object):\n",
    "    def __init__(self, init_value, name):\n",
    "        self.value = init_value\n",
    "        self.name = name\n",
    "        self.gradient = 0\n",
    "    def evaluate(self):\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.gradient += gradient\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class BinaryOperator(object):\n",
    "    def __init__(self, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "class Add(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() + self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient)\n",
    "        self.b.backpropagate(gradient)\n",
    "    def __str__(self):\n",
    "        return \"{} + {}\".format(self.a, self.b)\n",
    "\n",
    "class Mul(BinaryOperator):\n",
    "    def evaluate(self):\n",
    "        self.value = self.a.evaluate() * self.b.evaluate()\n",
    "        return self.value\n",
    "    def backpropagate(self, gradient):\n",
    "        self.a.backpropagate(gradient * self.b.value)\n",
    "        self.b.backpropagate(gradient * self.a.value)\n",
    "    def __str__(self):\n",
    "        return \"({}) * ({})\".format(self.a, self.b)\n",
    "\n",
    "x = Var(3, name=\"x\")\n",
    "y = Var(4, name=\"y\")\n",
    "f = Add(Mul(Mul(x, x), y), Add(y, Const(2))) # f(x,y) = xÂ²y + y + 2\n",
    "\n",
    "result = f.evaluate()\n",
    "f.backpropagate(1.0)\n",
    "\n",
    "print(\"f(x,y) =\", f)\n",
    "print(\"f(3,4) =\", result)\n",
    "print(\"df_dx =\", x.gradient)\n",
    "print(\"df_dy =\", y.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff â€“ reverse mode (using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "x = tf.Variable(3., name=\"x\")\n",
    "y = tf.Variable(4., name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "gradients = tf.gradients(f, [x, y])\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    f_val, gradients_val = sess.run([f, gradients])\n",
    "\n",
    "f_val, gradients_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reciprocal()` function from SciPy's `stats` module returns a random distribution that is commonly used when you have no idea of the optimal scale of a hyperparameter. See the exercise solutions for chapter 2 for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {
   "height": "603px",
   "width": "616px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
