{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Deep Neural Nets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEMCAYAAADEXsFmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f082eb0ccc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logit / Sigmoid function plot\n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW8AAAEMCAYAAAALXDfgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f080e7ddb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Leaky ReLU plot \n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)\n",
    "\n",
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing Leaky ReLU in TensorFlow\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a neural network on MNIST using the Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1- First let's create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2- Now let's load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9018\n",
      "5 Batch accuracy: 0.92 Validation accuracy: 0.9486\n",
      "10 Batch accuracy: 0.98 Validation accuracy: 0.9634\n",
      "15 Batch accuracy: 1.0 Validation accuracy: 0.9706\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9746\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.976\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.978\n",
      "35 Batch accuracy: 0.96 Validation accuracy: 0.9784\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: mnist.validation.images, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEOCAYAAABsJGdEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXkQkwv4/06HD9dVV0tAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f080cd88940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ELU plot\n",
    "\n",
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
    "\n",
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementing ELU in TensorFlow\n",
    "# Just specify the activation function when building each layer\n",
    "    \n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017 (I will definitely add it to the book). It outperforms the other activation functions very significantly for deep neural networks, so you should really try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEMCAYAAAAh7MZPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0808d096d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "# Check the mean and standard deviation in the deep layers for SELU activation function\n",
    "\n",
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a neural net for MNIST using the SELU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.96 Validation accuracy: 0.9244\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.9582\n",
      "10 Batch accuracy: 0.96 Validation accuracy: 0.9658\n",
      "15 Batch accuracy: 0.92 Validation accuracy: 0.9672\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9708\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9712\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.972\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.972\n"
     ]
    }
   ],
   "source": [
    "means = mnist.train.images.mean(axis=0, keepdims=True)\n",
    "stds = mnist.train.images.std(axis=0, keepdims=True) + 1e-10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            X_val_scaled = (mnist.validation.images - means) / stds\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_val_scaled, y: mnist.validation.labels})\n",
    "            print(epoch, \"Batch accuracy:\", acc_train, \"Validation accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's implement Batch Normalization with TensorFlow\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "\n",
    "# Set Training. Either a Python boolean, or a TensorFlow boolean scalar tensor (e.g. a placeholder)\n",
    "# Whether to return the output in training mode (normalized with statistics of the current batch) \n",
    "# or in inference mode (normalized with moving statistics)\n",
    "\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "\n",
    "# We will discuss momentum later in the course\n",
    "\n",
    "bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = tf.layers.batch_normalization(logits_before_bn, training=training,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In previous example, we passed parameters repeatedly to batch_normalization function\n",
    "# To avoid repeating the same parameters over and over again, we can use Python's partial() function \n",
    "# Let's improve previous example with partial() \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\")\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\")\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\")\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's build a neural net for MNIST\n",
    "# Using the ELU activation function and Batch Normalization at each layer\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8738\n",
      "1 Test accuracy: 0.8976\n",
      "2 Test accuracy: 0.9108\n",
      "3 Test accuracy: 0.9208\n",
      "4 Test accuracy: 0.9287\n",
      "5 Test accuracy: 0.9352\n",
      "6 Test accuracy: 0.9406\n",
      "7 Test accuracy: 0.9444\n",
      "8 Test accuracy: 0.947\n",
      "9 Test accuracy: 0.9491\n",
      "10 Test accuracy: 0.953\n",
      "11 Test accuracy: 0.9542\n",
      "12 Test accuracy: 0.9572\n",
      "13 Test accuracy: 0.959\n",
      "14 Test accuracy: 0.9602\n",
      "15 Test accuracy: 0.9605\n",
      "16 Test accuracy: 0.9618\n",
      "17 Test accuracy: 0.9633\n",
      "18 Test accuracy: 0.9636\n",
      "19 Test accuracy: 0.9644\n"
     ]
    }
   ],
   "source": [
    "# Execution phase\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "# When is_training is True the moving_mean and moving_variance need to be updated by running these commands\n",
    "# extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "# sess.run([train_op, extra_update_ops], ...)\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What!? That's not a great accuracy for MNIST. Of course, if you train for longer it will get much better accuracy, but with such a shallow network, Batch Norm and ELU are unlikely to have very positive impact: they shine mostly for much deeper nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing: notice that the list of trainable variables is shorter than the list of all global variables. This is because the moving averages are non-trainable variables. If you want to reuse a pretrained neural network (see below), you must not forget these non-trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List trainable variables\n",
    "\n",
    "[v.name for v in tf.trainable_variables()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List global variables\n",
    "\n",
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a simple neural net for MNIST and add gradient clipping\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we apply gradient clipping.\n",
    "# For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.5127\n",
      "1 Test accuracy: 0.8305\n",
      "2 Test accuracy: 0.8825\n",
      "3 Test accuracy: 0.9033\n",
      "4 Test accuracy: 0.9145\n",
      "5 Test accuracy: 0.9234\n",
      "6 Test accuracy: 0.9315\n",
      "7 Test accuracy: 0.9348\n",
      "8 Test accuracy: 0.9385\n",
      "9 Test accuracy: 0.9434\n",
      "10 Test accuracy: 0.9423\n",
      "11 Test accuracy: 0.9481\n",
      "12 Test accuracy: 0.9503\n",
      "13 Test accuracy: 0.9525\n",
      "14 Test accuracy: 0.9533\n",
      "15 Test accuracy: 0.9553\n",
      "16 Test accuracy: 0.9579\n",
      "17 Test accuracy: 0.959\n",
      "18 Test accuracy: 0.9593\n",
      "19 Test accuracy: 0.9611\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that you can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"model_ckps/my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/Const\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/Const\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/Const\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/Const\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/Const\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/Const\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/Const\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/InTopK\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/RestoreV2_1/tensor_names\n",
      "save/RestoreV2_1/shape_and_slices\n",
      "save/RestoreV2_1\n",
      "save/Assign_1\n",
      "save/RestoreV2_2/tensor_names\n",
      "save/RestoreV2_2/shape_and_slices\n",
      "save/RestoreV2_2\n",
      "save/Assign_2\n",
      "save/RestoreV2_3/tensor_names\n",
      "save/RestoreV2_3/shape_and_slices\n",
      "save/RestoreV2_3\n",
      "save/Assign_3\n",
      "save/RestoreV2_4/tensor_names\n",
      "save/RestoreV2_4/shape_and_slices\n",
      "save/RestoreV2_4\n",
      "save/Assign_4\n",
      "save/RestoreV2_5/tensor_names\n",
      "save/RestoreV2_5/shape_and_slices\n",
      "save/RestoreV2_5\n",
      "save/Assign_5\n",
      "save/RestoreV2_6/tensor_names\n",
      "save/RestoreV2_6/shape_and_slices\n",
      "save/RestoreV2_6\n",
      "save/Assign_6\n",
      "save/RestoreV2_7/tensor_names\n",
      "save/RestoreV2_7/shape_and_slices\n",
      "save/RestoreV2_7\n",
      "save/Assign_7\n",
      "save/RestoreV2_8/tensor_names\n",
      "save/RestoreV2_8/shape_and_slices\n",
      "save/RestoreV2_8\n",
      "save/Assign_8\n",
      "save/RestoreV2_9/tensor_names\n",
      "save/RestoreV2_9/shape_and_slices\n",
      "save/RestoreV2_9\n",
      "save/Assign_9\n",
      "save/RestoreV2_10/tensor_names\n",
      "save/RestoreV2_10/shape_and_slices\n",
      "save/RestoreV2_10\n",
      "save/Assign_10\n",
      "save/RestoreV2_11/tensor_names\n",
      "save/RestoreV2_11/shape_and_slices\n",
      "save/RestoreV2_11\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a `FileWriter` to save the graph and then visualize it in TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/InTopK&quot;\\n  op: &quot;InTopK&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT64\\n    }\\n  }\\n  attr {\\n    key: &quot;k&quot;\\n    value {\\n      i: 1\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/InTopK&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_1&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_1/tensor_names&quot;\\n  input: &quot;save/RestoreV2_1/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_2/tensor_names&quot;\\n  input: &quot;save/RestoreV2_2/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_3&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_3/tensor_names&quot;\\n  input: &quot;save/RestoreV2_3/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_4&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_4/tensor_names&quot;\\n  input: &quot;save/RestoreV2_4/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_5&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_5/tensor_names&quot;\\n  input: &quot;save/RestoreV2_5/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_6&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_6/tensor_names&quot;\\n  input: &quot;save/RestoreV2_6/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_7&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_7/tensor_names&quot;\\n  input: &quot;save/RestoreV2_7/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_8&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_8/tensor_names&quot;\\n  input: &quot;save/RestoreV2_8/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_9&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_9/tensor_names&quot;\\n  input: &quot;save/RestoreV2_9/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_10&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_10/tensor_names&quot;\\n  input: &quot;save/RestoreV2_10/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2_11&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2_11/tensor_names&quot;\\n  input: &quot;save/RestoreV2_11/shape_and_slices&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a session, restore the model's state and continue training on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let's test this for real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9609\n",
      "1 Test accuracy: 0.9608\n",
      "2 Test accuracy: 0.9617\n",
      "3 Test accuracy: 0.9613\n",
      "4 Test accuracy: 0.9639\n",
      "5 Test accuracy: 0.9649\n",
      "6 Test accuracy: 0.9663\n",
      "7 Test accuracy: 0.9627\n",
      "8 Test accuracy: 0.9665\n",
      "9 Test accuracy: 0.9669\n",
      "10 Test accuracy: 0.9662\n",
      "11 Test accuracy: 0.9674\n",
      "12 Test accuracy: 0.9678\n",
      "13 Test accuracy: 0.9679\n",
      "14 Test accuracy: 0.9688\n",
      "15 Test accuracy: 0.9684\n",
      "16 Test accuracy: 0.9687\n",
      "17 Test accuracy: 0.9702\n",
      "18 Test accuracy: 0.9673\n",
      "19 Test accuracy: 0.9687\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9611\n",
      "1 Test accuracy: 0.9619\n",
      "2 Test accuracy: 0.9622\n",
      "3 Test accuracy: 0.9619\n",
      "4 Test accuracy: 0.9644\n",
      "5 Test accuracy: 0.9633\n",
      "6 Test accuracy: 0.9647\n",
      "7 Test accuracy: 0.9648\n",
      "8 Test accuracy: 0.9671\n",
      "9 Test accuracy: 0.9677\n",
      "10 Test accuracy: 0.9676\n",
      "11 Test accuracy: 0.9679\n",
      "12 Test accuracy: 0.9687\n",
      "13 Test accuracy: 0.9688\n",
      "14 Test accuracy: 0.9683\n",
      "15 Test accuracy: 0.9693\n",
      "16 Test accuracy: 0.9677\n",
      "17 Test accuracy: 0.9697\n",
      "18 Test accuracy: 0.9692\n",
      "19 Test accuracy: 0.9707\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you will want to reuse only the lower layers. If you are using `import_meta_graph()` it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"model_ckps/my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9142\n",
      "1 Test accuracy: 0.9346\n",
      "2 Test accuracy: 0.9437\n",
      "3 Test accuracy: 0.9486\n",
      "4 Test accuracy: 0.9517\n",
      "5 Test accuracy: 0.9544\n",
      "6 Test accuracy: 0.9544\n",
      "7 Test accuracy: 0.9562\n",
      "8 Test accuracy: 0.9588\n",
      "9 Test accuracy: 0.9619\n",
      "10 Test accuracy: 0.9617\n",
      "11 Test accuracy: 0.9617\n",
      "12 Test accuracy: 0.9624\n",
      "13 Test accuracy: 0.9644\n",
      "14 Test accuracy: 0.9622\n",
      "15 Test accuracy: 0.964\n",
      "16 Test accuracy: 0.9666\n",
      "17 Test accuracy: 0.9668\n",
      "18 Test accuracy: 0.9673\n",
      "19 Test accuracy: 0.9687\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you must create one `Saver` to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another `Saver` to save the new model, once it is trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9022\n",
      "1 Test accuracy: 0.9302\n",
      "2 Test accuracy: 0.9393\n",
      "3 Test accuracy: 0.9429\n",
      "4 Test accuracy: 0.9484\n",
      "5 Test accuracy: 0.9511\n",
      "6 Test accuracy: 0.9517\n",
      "7 Test accuracy: 0.9539\n",
      "8 Test accuracy: 0.9545\n",
      "9 Test accuracy: 0.9572\n",
      "10 Test accuracy: 0.9599\n",
      "11 Test accuracy: 0.9602\n",
      "12 Test accuracy: 0.9606\n",
      "13 Test accuracy: 0.9619\n",
      "14 Test accuracy: 0.9619\n",
      "15 Test accuracy: 0.9636\n",
      "16 Test accuracy: 0.9633\n",
      "17 Test accuracy: 0.9643\n",
      "18 Test accuracy: 0.9651\n",
      "19 Test accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                      # not shown in the book\n",
    "        for iteration in range(mnist.train.num_examples // batch_size): # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)      # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})  # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,  # not shown\n",
    "                                                y: mnist.test.labels}) # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Models from Other Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, for each variable we want to reuse, we find its initializer's assignment operation, and we get its second input, which corresponds to the initialization value. When we run the initializer, we replace the initialization values with the ones we want, using a `feed_dict`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the assignment nodes for the hidden1 variables\n",
    "graph = tf.get_default_graph()\n",
    "assign_kernel = graph.get_operation_by_name(\"hidden1/kernel/Assign\")\n",
    "assign_bias = graph.get_operation_by_name(\"hidden1/bias/Assign\")\n",
    "init_kernel = assign_kernel.inputs[1]\n",
    "init_bias = assign_bias.inputs[1]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init, feed_dict={init_kernel: original_w, init_bias: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))  # not shown in the book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the weights variable created by the `tf.layers.dense()` function is called `\"kernel\"` (instead of `\"weights\"` when using the `tf.contrib.layers.fully_connected()`, as in the book), and the biases variable is called `bias` instead of `biases`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach (initially used in the book) would be to create dedicated assignment nodes and dedicated placeholders. This is more verbose and less efficient, but you may find this more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  61.   83.  105.]]\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 2\n",
    "n_hidden1 = 3\n",
    "\n",
    "original_w = [[1., 2., 3.], [4., 5., 6.]] # Load the weights from the other framework\n",
    "original_b = [7., 8., 9.]                 # Load the biases from the other framework\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "# [...] Build the rest of the model\n",
    "\n",
    "# Get a handle on the variables of layer hidden1\n",
    "with tf.variable_scope(\"\", default_name=\"\", reuse=True):  # root scope\n",
    "    hidden1_weights = tf.get_variable(\"hidden1/kernel\")\n",
    "    hidden1_biases = tf.get_variable(\"hidden1/bias\")\n",
    "\n",
    "# Create dedicated placeholders and assignment nodes\n",
    "original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))\n",
    "original_biases = tf.placeholder(tf.float32, shape=n_hidden1)\n",
    "assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)\n",
    "assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})\n",
    "    sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})\n",
    "    # [...] Train the model on your new task\n",
    "    print(hidden1.eval(feed_dict={X: [[10.0, 11.0]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could also get a handle on the variables using `get_collection()` and specifying the `scope`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/bias:0' shape=(3,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could use the graph's `get_tensor_by_name()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/kernel:0' shape=(2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'hidden1/bias:0' shape=(3,) dtype=float32_ref>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_default_graph().get_tensor_by_name(\"hidden1/bias:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.8987\n",
      "1 Test accuracy: 0.9311\n",
      "2 Test accuracy: 0.9375\n",
      "3 Test accuracy: 0.9414\n",
      "4 Test accuracy: 0.9437\n",
      "5 Test accuracy: 0.9479\n",
      "6 Test accuracy: 0.9495\n",
      "7 Test accuracy: 0.9521\n",
      "8 Test accuracy: 0.9517\n",
      "9 Test accuracy: 0.9525\n",
      "10 Test accuracy: 0.9535\n",
      "11 Test accuracy: 0.9538\n",
      "12 Test accuracy: 0.9534\n",
      "13 Test accuracy: 0.9546\n",
      "14 Test accuracy: 0.9538\n",
      "15 Test accuracy: 0.9553\n",
      "16 Test accuracy: 0.9552\n",
      "17 Test accuracy: 0.9549\n",
      "18 Test accuracy: 0.9553\n",
      "19 Test accuracy: 0.9557\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is exactly the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9031\n",
      "1 Test accuracy: 0.932\n",
      "2 Test accuracy: 0.94\n",
      "3 Test accuracy: 0.9435\n",
      "4 Test accuracy: 0.9473\n",
      "5 Test accuracy: 0.9492\n",
      "6 Test accuracy: 0.9498\n",
      "7 Test accuracy: 0.9493\n",
      "8 Test accuracy: 0.9515\n",
      "9 Test accuracy: 0.9519\n",
      "10 Test accuracy: 0.9529\n",
      "11 Test accuracy: 0.9536\n",
      "12 Test accuracy: 0.9529\n",
      "13 Test accuracy: 0.9532\n",
      "14 Test accuracy: 0.9522\n",
      "15 Test accuracy: 0.9534\n",
      "16 Test accuracy: 0.953\n",
      "17 Test accuracy: 0.955\n",
      "18 Test accuracy: 0.955\n",
      "19 Test accuracy: 0.9552\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "reuse_vars_dict = dict([(var.op.name, var) for var in reuse_vars])\n",
    "restore_saver = tf.train.Saver(reuse_vars_dict) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Test accuracy: 0.9033\n",
      "1 Test accuracy: 0.9322\n",
      "2 Test accuracy: 0.9423\n",
      "3 Test accuracy: 0.9449\n",
      "4 Test accuracy: 0.9471\n",
      "5 Test accuracy: 0.9477\n",
      "6 Test accuracy: 0.951\n",
      "7 Test accuracy: 0.9507\n",
      "8 Test accuracy: 0.9514\n",
      "9 Test accuracy: 0.9522\n",
      "10 Test accuracy: 0.9512\n",
      "11 Test accuracy: 0.9521\n",
      "12 Test accuracy: 0.9522\n",
      "13 Test accuracy: 0.9539\n",
      "14 Test accuracy: 0.9536\n",
      "15 Test accuracy: 0.9534\n",
      "16 Test accuracy: 0.9547\n",
      "17 Test accuracy: 0.9537\n",
      "18 Test accuracy: 0.9542\n",
      "19 Test accuracy: 0.9547\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = mnist.train.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"model_ckps/my_model_final.ckpt\")\n",
    "    \n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})\n",
    "    h2_cache_test = sess.run(hidden2, feed_dict={X: mnist.test.images}) # not shown in the book\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(mnist.train.num_examples)\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_test, # not shown\n",
    "                                                y: mnist.test.labels})  # not shown\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)                    # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9579\n",
      "1 Test accuracy: 0.9691\n",
      "2 Test accuracy: 0.976\n",
      "3 Test accuracy: 0.9793\n",
      "4 Test accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is just as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8343\n",
      "1 Test accuracy: 0.8726\n",
      "2 Test accuracy: 0.8832\n",
      "3 Test accuracy: 0.8899\n",
      "4 Test accuracy: 0.8958\n",
      "5 Test accuracy: 0.8986\n",
      "6 Test accuracy: 0.9011\n",
      "7 Test accuracy: 0.9032\n",
      "8 Test accuracy: 0.9046\n",
      "9 Test accuracy: 0.9047\n",
      "10 Test accuracy: 0.9065\n",
      "11 Test accuracy: 0.9059\n",
      "12 Test accuracy: 0.9072\n",
      "13 Test accuracy: 0.9072\n",
      "14 Test accuracy: 0.9069\n",
      "15 Test accuracy: 0.9071\n",
      "16 Test accuracy: 0.9064\n",
      "17 Test accuracy: 0.9071\n",
      "18 Test accuracy: 0.9068\n",
      "19 Test accuracy: 0.9063\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a regularization function to the `tf.layers.dense()` function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Python's `partial()` function to avoid repeating the same arguments over and over again. Note that we set the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.8298\n",
      "1 Test accuracy: 0.8778\n",
      "2 Test accuracy: 0.8917\n",
      "3 Test accuracy: 0.9017\n",
      "4 Test accuracy: 0.9068\n",
      "5 Test accuracy: 0.9103\n",
      "6 Test accuracy: 0.9125\n",
      "7 Test accuracy: 0.9137\n",
      "8 Test accuracy: 0.9149\n",
      "9 Test accuracy: 0.9174\n",
      "10 Test accuracy: 0.9176\n",
      "11 Test accuracy: 0.9184\n",
      "12 Test accuracy: 0.9191\n",
      "13 Test accuracy: 0.9183\n",
      "14 Test accuracy: 0.9195\n",
      "15 Test accuracy: 0.9201\n",
      "16 Test accuracy: 0.9181\n",
      "17 Test accuracy: 0.9184\n",
      "18 Test accuracy: 0.9181\n",
      "19 Test accuracy: 0.9174\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n",
    "* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n",
    "* the `is_training` parameter is renamed to `training`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9205\n",
      "1 Test accuracy: 0.9418\n",
      "2 Test accuracy: 0.9486\n",
      "3 Test accuracy: 0.9508\n",
      "4 Test accuracy: 0.954\n",
      "5 Test accuracy: 0.957\n",
      "6 Test accuracy: 0.9604\n",
      "7 Test accuracy: 0.9585\n",
      "8 Test accuracy: 0.9598\n",
      "9 Test accuracy: 0.9663\n",
      "10 Test accuracy: 0.9644\n",
      "11 Test accuracy: 0.9646\n",
      "12 Test accuracy: 0.9675\n",
      "13 Test accuracy: 0.9657\n",
      "14 Test accuracy: 0.9645\n",
      "15 Test accuracy: 0.9668\n",
      "16 Test accuracy: 0.969\n",
      "17 Test accuracy: 0.9682\n",
      "18 Test accuracy: 0.9698\n",
      "19 Test accuracy: 0.9682\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the `clip_by_norm()` function. Then we create an assignment operation to assign the clipped weights to the weights variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this as well for the second hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an initializer and a saver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the model. It's pretty much as usual, except that right after running the `training_op`, we run the `clip_weights` and `clip_weights2` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9517\n",
      "1 Test accuracy: 0.9674\n",
      "2 Test accuracy: 0.9712\n",
      "3 Test accuracy: 0.9759\n",
      "4 Test accuracy: 0.975\n",
      "5 Test accuracy: 0.9761\n",
      "6 Test accuracy: 0.9765\n",
      "7 Test accuracy: 0.9796\n",
      "8 Test accuracy: 0.9791\n",
      "9 Test accuracy: 0.9794\n",
      "10 Test accuracy: 0.9805\n",
      "11 Test accuracy: 0.9809\n",
      "12 Test accuracy: 0.9807\n",
      "13 Test accuracy: 0.9799\n",
      "14 Test accuracy: 0.982\n",
      "15 Test accuracy: 0.9816\n",
      "16 Test accuracy: 0.9825\n",
      "17 Test accuracy: 0.9825\n",
      "18 Test accuracy: 0.9816\n",
      "19 Test accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):  # not shown\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)       # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,       # not shown\n",
    "                                            y: mnist.test.labels})      # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                        # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a `max_norm_regularizer()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is as usual, except you must run the weights clipping operations after each training operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Test accuracy: 0.9527\n",
      "1 Test accuracy: 0.9653\n",
      "2 Test accuracy: 0.97\n",
      "3 Test accuracy: 0.9751\n",
      "4 Test accuracy: 0.9752\n",
      "5 Test accuracy: 0.9742\n",
      "6 Test accuracy: 0.9754\n",
      "7 Test accuracy: 0.9784\n",
      "8 Test accuracy: 0.9775\n",
      "9 Test accuracy: 0.9789\n",
      "10 Test accuracy: 0.9808\n",
      "11 Test accuracy: 0.9797\n",
      "12 Test accuracy: 0.9802\n",
      "13 Test accuracy: 0.9799\n",
      "14 Test accuracy: 0.9808\n",
      "15 Test accuracy: 0.9809\n",
      "16 Test accuracy: 0.9807\n",
      "17 Test accuracy: 0.9803\n",
      "18 Test accuracy: 0.9816\n",
      "19 Test accuracy: 0.9812\n"
     ]
    }
   ],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images,     # not shown in the book\n",
    "                                            y: mnist.test.labels})    # not shown\n",
    "        print(epoch, \"Test accuracy:\", acc_test)                      # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"model_ckps/my_model_final.ckpt\")             # not shown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
